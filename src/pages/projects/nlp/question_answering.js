import * as React from 'react'
import { ProjectPage } from '../../../components/ProjectPage'

import '../../../styles/app.css'

// Images

import approach1 from '../../../images/nlp/question_answering/approach1.png';
import approach2 from '../../../images/nlp/question_answering/approach2.png';
import evaluation from '../../../images/nlp/question_answering/evaluation.png';
import example from '../../../images/nlp/question_answering/example.png';
import ranker from '../../../images/nlp/question_answering/ranker.png';
import sanity from '../../../images/nlp/question_answering/sanity.png';

const QAPage = () => {
  return (
    <ProjectPage image={ example } title="Question Answering" >    
      <article className="project-article">
        <h1>Task</h1>
        <p>The goal of this project is to build a Question Answering system. The input is a passage (from Wikipedia) along with a set of 3 questions. Each question has only one correct answer, which must be returned as a phrase or a complete sentence. There should also not be any (too much) redundunt information in the outputted answers.</p>
      </article>
      <article className="project-article">
        <h1>Data Cleaning</h1>
        <p>The data cleanning procedure used for the "Question Generation" project was also used here (including the sentence tokenization step). Please refer to details there.</p>
      </article>
      <article className="project-article">
        <h1>Approach</h1>
        <p>We began with basic pre-trained BERT model with one additional linear layer for predicting the start and end indices of the answer in the passage.</p>
        <img src={approach1} alt="Approach Part #1" /> 
        <p>We then processed the SQuAD 2.0 dataset and tokenized it. We also processed &amp; tokenized an additional dev dataset (provided by our instructors). Using these two, we fine-tuned the BERT model to provide answers given the context (passage) and one question at a time. </p>
        <img src={approach2} alt="Approach Part #2" /> 
      </article>
      <article className="project-article">
        <h1>Sanity Check</h1>
        <p>To make sure that our system was generating valid answers, we checked them against a set of reference answers. As seen below, the answers were pretty good. The only problem was that our system was extremely slow since the passages were quite long.</p>
        <img src={sanity} alt="Sanity Check" /> 
      </article>
      <article className="project-article">
        <h1>Sentence Ranking</h1>
        <p>To fix the issue mentioned earlier, we decided to build an additional model to score sentences in the passage based on how relevant they are to a question (another fine-tuned BERT model was used). We then only use the top 5 most relevant sentences as a context to the Question Answering system.</p>
        <img className="w40" src={ranker} alt="Sentence Relevance Ranker" /> 
      </article>
      <article className="project-article">
        <h1>Evaluation</h1>
        <p>We used 3 metrics to evaluate the answers generated by our system (descriptions in the table below). For each, we outputted a score between 0-1 and averaged the scores to obtain a final number (the BLEU score was given a higher weightage). (Note: This number isn't like a typical "accuracy" score that you can use to compare our system against others; hence, the value we obtained is not mentioned. The only purpose of this number was for us to ensure that our system was improving after we had made any modifications.)</p>
        <img src={evaluation} alt="Evaluation Metrics" /> 
      </article>

    </ProjectPage>
  )
}

export default QAPage
